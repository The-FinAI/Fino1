{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e486328a-9b04-49e7-8b98-b77271d39865",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ca5e3-f8a2-42b7-9fc0-e9776dd98030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('econ logic qa data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97895e7b-a086-4692-9f9f-410dbc1bd9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37de761-9fb8-411a-8b76-3e6fddabed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = []\n",
    "answer = []\n",
    "\n",
    "prompt_template = \"\"\"Given a specific process, event, or scenario, along with a set of unordered options, arrange them into a logically ordered sequence. \n",
    "The sequence should follow a natural progression where each step builds upon the previous one.\n",
    "Question: {}\n",
    "Options: \n",
    "A: {}\n",
    "B: {}\n",
    "C: {}\n",
    "D: {}\n",
    "Answer:\"\"\"\n",
    "\n",
    "for i in data['train']:\n",
    "    query.append(prompt_template.format(i['Question'], i['A'], i['B'], i['C'], i['D']))\n",
    "    answer.append(i['Answer'])\n",
    "    #print(formatted_prompt)  # Print or store it in a list for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24736c-f512-471d-a1c2-f709d1d4d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'query': query, 'text': data['train']['Question'],'answer': answer, 'query_code': 'No', 'program': 'No'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8a575-8820-459d-b197-43c937cf1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760bc294-6c3e-447a-a76e-4976d111d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# 将 Pandas DataFrame 转换为 Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 定义数据集 repo 名称\n",
    "dataset_repo = \"XXXX/XXXo1\"\n",
    "\n",
    "# 推送数据集到 Hugging Face Hub\n",
    "hf_dataset.push_to_hub(dataset_repo)\n",
    "\n",
    "print(f\"Dataset uploaded to: https://huggingface.co/datasets/{dataset_repo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b8213-1473-4cbc-8b7b-e0cb58b4ede5",
   "metadata": {},
   "source": [
    "# filter-step1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123cb4a0-6a11-4e50-acee-c967dc1ddb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import traceback\n",
    "# 设置 Hugging Face API Token\n",
    "os.environ[\"OPENAI_API_SECRET_KEY\"] = \"your openai api key here\"\n",
    "\n",
    "os.environ[\"DEEPSEEK_API_SECRET_KEY\"] = \"your deepseek api key here\"\n",
    "os.environ['OPENAI_URL'] = \"https://api.openai.com/v1/chat/completions\"\n",
    "os.environ['DEEPSEEK_URL'] = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"your hugging face token here\"\n",
    "\n",
    "class GPT:\n",
    "    def __init__(self, model_name, api_url, api_key):\n",
    "        self.model_name = model_name\n",
    "        self.api_url = api_url\n",
    "        self.api_key = api_key\n",
    "        print(f\"Using model: {self.model_name}\")\n",
    "\n",
    "    def call(self, content, additional_args={}):\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [{'role': 'user', 'content': content}],\n",
    "            **additional_args,\n",
    "        }\n",
    "        response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "        response_data = response.json()\n",
    "\n",
    "        if 'error' in response_data:\n",
    "            raise ValueError(f\"API Error: {response_data}\")\n",
    "\n",
    "        return response_data['choices'][0]['message']['content']\n",
    "\n",
    "    @retry(wait=wait_fixed(3), stop=stop_after_attempt(3))\n",
    "    def retry_call(self, content, additional_args={\"max_tokens\": 8192}):\n",
    "        return self.call(content, additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24705c-1115-4506-ac64-1a7cef719424",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35 = GPT(model_name='gpt-4o-mini', api_url=os.environ['OPENAI_URL'], api_key=os.environ[\"OPENAI_API_SECRET_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e078c8f-dea8-4041-809e-00e88e71883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35.retry_call('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7c3ed-3a52-4881-9994-7d3aa7d8dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_prompt = \"\"\"<Model Response>  \n",
    "{}  \n",
    "</Model Response>  \n",
    "\n",
    "<Reference Answer>  \n",
    "{}\n",
    "</Reference Answer> \n",
    "\n",
    "You are provided with a model-generated response (<Model Response>) and a reference answer (<Reference Answer>). Compare the model response with the reference answer and determine its correctness. Please be mercy when judging. Your task is to simply output \"True\" if the response is correct, and \"False\" otherwise.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05319a-a389-4477-8be2-3fcefb90cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_gpt(conclusion,answer):\n",
    "    query = verify_prompt.format(conclusion,answer)\n",
    "    response = gpt_35.retry_call(query)\n",
    "    if 'true' in response.lower():\n",
    "#        d['verify'].append(True)\n",
    "        return True\n",
    "    else:\n",
    "#        d['verify'].append(False)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a479b4f-cd8a-4c11-95b4-0fa532e70e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name = 'your jsonl dataset path' # For example o1-step-1-result/samples_o1_step1_2025-02-18T05-29-03.441995.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224223a-e46e-49b3-9cd4-9bd5ecf720da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(file_name, 'r') as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line.strip())\n",
    "        data.append(item)\n",
    "gold = [item['target'] for item in data]\n",
    "predict = [item['resps'][0][0].split('<|eot_id|>')[0] for item in data]\n",
    "inputs = [item['doc']['query'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd3c8c-92a0-418f-9800-48bd992e1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "verify = []\n",
    "for pre,gol in tqdm(zip(predict, gold)):\n",
    "    verify.append(verify_gpt(pre, gol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63a052-240c-4987-a701-aa2be2aa1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"inputs\": inputs,\n",
    "    \"gold\": gold,\n",
    "    \"predict\": predict,\n",
    "    \"verify\": verify\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86403ba-9bd3-40f5-a6b7-b4309518edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('verified_o1_step1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a3722-ef69-4283-81f8-a5c78ae96bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correct = df[df.verify == True].reset_index(drop=True)\n",
    "df_wrong = df[df.verify == False].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc1dd6-7026-49ae-bf51-7b6885d4c82d",
   "metadata": {},
   "source": [
    "# merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dd022-4e10-4b47-a722-c5e3cb0be6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "df_finqa = pd.DataFrame(load_dataset('FINQA test data')['train'])[['query', 'answer', 'query_code', 'program', 'text']]\n",
    "df_dm = pd.DataFrame(load_dataset('DocMath-Eval data')['train'])[['query', 'answer', 'query_code', 'program', 'text']]\n",
    "df_tat = pd.DataFrame(load_dataset('FLARE TATQA test data')['train'])[['query', 'answer', 'query_code', 'program', 'text']]\n",
    "\n",
    "df_docfinqa = pd.DataFrame(load_dataset('DocFinQA data')['train'])[['query', 'answer', 'query_code', 'program', 'text']]\n",
    "df_bizqa = pd.DataFrame(load_dataset('bizbench QA data')['train'])[['qucry', 'answer', 'query_code', 'program', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd8d06-7898-4481-ae9b-dc02fb9c834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docfinqa = df_docfinqa[df_docfinqa.program=='No'].reset_index(drop=True)\n",
    "df_bizqa = df_bizqa[df_bizqa.program=='No'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efba43-bd42-44bc-b25f-ceed1508aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_finqa, df_dm, df_tat, df_logic, df_docfinqa[:1000],df_bizqa[:1000]], ignore_index=True)\n",
    "df_filtered = df_combined[df_combined['answer'].astype(str).str.len() <= 10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728ee34-5c57-4866-8391-4f720e2131df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('o1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787e861-cd36-4d84-838c-ca4deb9a69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('verified_o1_step1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305ec6b-7b98-473e-9275-6e0805e44015",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2result = {}\n",
    "for i in df.iloc:\n",
    "    query2result[i['inputs']] = i['verify']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d2574-33a6-4d92-82d4-656ad24afa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify = []\n",
    "for i in df_filtered.iloc:\n",
    "    if i['query'] in query2result:\n",
    "        verify.append(query2result[i['query']])\n",
    "    else:\n",
    "        verify.append(False)\n",
    "\n",
    "df_filtered['verify'] = verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35e4bb-cd4d-4fad-824e-1e5968c6f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered.verify==False].reset_index(drop=True)\n",
    "df_filtered = df_filtered[df_filtered['answer'].astype(str).str.len() <= 10].reset_index(drop=True)\n",
    "df_filtered = df_filtered.drop(['query_code', 'program'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171ceb2-1fc4-4d97-b0c1-6c85768299be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('filtered_o1_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058f4ee-d7b3-4ff3-afd6-adcf8e03ad46",
   "metadata": {},
   "source": [
    "# merge with CONVFINQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6aa40-3aa6-4ab8-a9d5-cca293c49676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = df_filtered #pd.read_csv('filtered_o1_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51e309-06a6-4cfa-bdf3-cd4b6c045e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "conv = pd.DataFrame(load_dataset('XXXX/convfinqa')['train'])\n",
    "\n",
    "conv.columns = ['id', 'query','answer', 'turn', 'dialogue_id']\n",
    "df_max_turn = conv.loc[conv.groupby('dialogue_id')['turn'].idxmax()].reset_index(drop=True)\n",
    "df_max_turn['conver'] = df_max_turn['query'].apply(lambda x: x.split('\\nConversations: \\n')[1])\n",
    "\n",
    "df = pd.concat([df, df_max_turn])\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.fillna('No')\n",
    "df = df.drop('id,turn,dialogue_id'.split(','),axis=1)\n",
    "df['query'] = df['query'].apply(lambda x: x.split('\\nConversations: \\n')[0])\n",
    "df = df.drop(['verify'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7f579-a6c1-480b-85bc-580ed0de425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined=df_combined.fillna('No')\n",
    "df_combined.to_csv('conv_and_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd1992-ed7c-42d9-aa57-2d36c9de9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 提取 500-700 字符片段\n",
    "def extract_substring(text):\n",
    "    return text[500:] if len(text) > 700 else text[500:]\n",
    "\n",
    "df_combined[\"substring\"] = df_combined[\"query\"].apply(extract_substring)\n",
    "\n",
    "# 找到重复的片段，只保留最后一个\n",
    "df_filtered = df_combined[~df_combined.duplicated(subset=[\"substring\"], keep=\"last\")].drop(columns=[\"substring\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46c388-fce9-4a2e-8097-8950769bb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cc552-191b-4dd8-ad93-a915601f86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('conv_and_filtered_remove_duplicated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6337d-93ce-4876-a62a-cfdf9db8a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_filtered['query'][0]) #['query'][0][500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e96af3-0979-443c-a324-28cdc2aef338",
   "metadata": {},
   "source": [
    "# filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b91f86-47cb-4d18-a356-2590240966b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_filtered = pd.read_csv('conv_and_filtered_remove_duplicated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb082b-40c0-4474-856f-cdec39e52729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def n_gram_similarity(str1, str2, n=2):\n",
    "    \"\"\"计算两个字符串的 N-gram 相似度\"\"\"\n",
    "    if len(str1) < n or len(str2) < n:\n",
    "        return 0.0\n",
    "    \n",
    "    set1 = {str1[i:i+n] for i in range(len(str1) - n + 1)}\n",
    "    set2 = {str2[i:i+n] for i in range(len(str2) - n + 1)}\n",
    "    \n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"计算两个字符串的 Jaccard 相似度\"\"\"\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    \n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def deduplicate_queries(df_filtered, query_column=\"query\", similarity_threshold=0.8, similarity_func=n_gram_similarity):\n",
    "    \"\"\"在相似的 query 中保留最后一个出现的 query\"\"\"\n",
    "    if query_column not in df_filtered.columns:\n",
    "        raise ValueError(f\"Column '{query_column}' not found in DataFrame\")\n",
    "    \n",
    "    df_filtered = df_filtered.copy()\n",
    "    df_filtered[\"index\"] = df_filtered.index  # 保留原始索引\n",
    "    df_filtered.sort_index(ascending=False, inplace=True)  # 按索引降序排列\n",
    "    \n",
    "    unique_queries = []\n",
    "    removed_indices = set()\n",
    "    \n",
    "    for i, row in tqdm(df_filtered.iterrows()):\n",
    "        if i in removed_indices:\n",
    "            continue\n",
    "        \n",
    "        current_query = row[query_column]\n",
    "        \n",
    "        for j, compare_row in df_filtered.iterrows():\n",
    "            #continue\n",
    "            if i == j or j in removed_indices:\n",
    "                continue\n",
    "            \n",
    "            compare_query = compare_row[query_column]\n",
    "            similarity = similarity_func(current_query, compare_query)\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                removed_indices.add(j)  # 移除相似的较早出现的 query\n",
    "        \n",
    "        unique_queries.append(row)\n",
    "    \n",
    "    df_result = pd.DataFrame(unique_queries).sort_index()\n",
    "    df_result.drop(columns=[\"index\"], inplace=True)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# 示例用法\n",
    "df = pd.DataFrame(df_filtered)\n",
    "#df_filtered_jaccard = deduplicate_queries(df, query_column=\"query\", similarity_threshold=0.7, similarity_func=jaccard_similarity)\n",
    "df_filtered_ngram = deduplicate_queries(df, query_column=\"query\", similarity_threshold=0.7, similarity_func=lambda s1, s2: n_gram_similarity(s1, s2, n=2))\n",
    "\n",
    "# print(\"Jaccard 去重结果:\")\n",
    "# print(df_filtered_jaccard)\n",
    "# print(\"\\nN-gram 去重结果:\")\n",
    "# print(df_filtered_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f9d65-23a0-459d-bbf5-211225ad5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=[]\n",
    "for i in df_filtered['query']:\n",
    "    if \"Question\" not in i:\n",
    "        #print(i)\n",
    "        flag.append(1)\n",
    "    else:\n",
    "        flag.append(0)\n",
    "df_filtered['flag'] = flag\n",
    "df_filtered0 = df_filtered[df_filtered['flag']==0]\n",
    "\n",
    "df_filtered1 = df_filtered[df_filtered['flag']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a33e86-8bde-451e-bebb-3be3873f02af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "api_key=\"your openai api key here\"\n",
    "\n",
    "client_openai = openai.OpenAI(api_key=api_key)\n",
    "def merge_questions_with_gpt4o(questions: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses GPT-4o to merge decomposed questions into the original question.\n",
    "\n",
    "    Parameters:\n",
    "        questions (str): A string containing a series of decomposed questions.\n",
    "        api_key (str): Your OpenAI API key.\n",
    "\n",
    "    Returns:\n",
    "        str: The merged original question.\n",
    "    \"\"\"\n",
    "    prompt = '''i will give you a series of questions, these questions are a list of decomposed questions for the original question, \n",
    "    please help me merge all these questions into the original question. Do not mentioned any of the answers in the output. The fist pirorty is to make sure the answer to your questions is the only and exact the answer for the last question. Only return the merged original question.'''\n",
    "\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": questions}\n",
    "        ],\n",
    "            )\n",
    "    print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6751402-b5c1-465e-961d-b2959103d522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered1['text'] = df_filtered1['conver'].apply(lambda x: merge_questions_with_gpt4o(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d752beb-e806-45d6-98cc-61226d1e7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered1[\"query\"] = df_filtered1[\"query\"] + \"\\nQuestion: \" + df_filtered1[\"text\"] + \"\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50905782-b81e-4fe7-ac40-88afa2109912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered1 = df_filtered1[['query', 'answer', 'text', 'conver','flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d0f0e-a388-4e63-a33f-0355862f9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered0['text'] = df_filtered0['query'].apply(lambda x: x.split('Question')[1].split('Answer')[0].replace(':','').replace(' ','').replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb2f9f-22bf-4c64-b744-4ea7e4d6a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered00 = df_filtered0.drop_duplicates(subset=['text', 'answer'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db77fc-8536-4320-aab6-ccdb3aea4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.concat([df_filtered00,df_filtered1])\n",
    "df_filtered = df_filtered.drop('flag',axis=1)\n",
    "df_filtered = df_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10072b5-f565-4d1d-95e6-f41457830714",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = {}\n",
    "for i in df_filtered00['query']:\n",
    "    maps[i.replace('\\t', '')] = i.replace('\\t', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1947d-3331-4bb2-9add-a4975e7611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['query'] = df_filtered['query'].apply(lambda x:x.replace('\\t', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540b18a-9a4f-4828-ad34-584c16086500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.columns = ['Open-ended Verifiable Question', 'Ground-True Answer', 'question', 'conver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4d2c7-5912-4249-8b8a-64efa2575827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['question'] = df_filtered['Open-ended Verifiable Question'].apply(lambda x: x.split('Question')[1].split('Answer')[0].replace(':',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd06aa6-d69d-47fa-b415-4a968c227c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('conv_and_filtered_remove_duplicated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385a51d-b659-48c6-8d3e-c3a618d2b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 df 是你的 DataFrame\n",
    "# 计算 token 数量（假设 token 以空格分隔）\n",
    "df_filtered[\"token_count\"] = df_filtered[\"Open-ended Verifiable Question\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# 过滤掉 token 数量超过 20000 的行\n",
    "df_filtered1 = df_filtered[df_filtered[\"token_count\"] <= 10000].reset_index(drop=True)\n",
    "\n",
    "# 删除临时列\n",
    "df_filtered1 = df_filtered1.drop(columns=[\"token_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0b1e7-aa83-4538-82d4-4fb05507f813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026ebda-d203-42d6-9115-9c8d1ec4e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered1.to_csv('conv_and_filtered_remove_duplicated_reomve_long.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcac3c9-35ef-4630-9764-608b729a2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79098449-1b62-4d32-b61f-076e378864a8",
   "metadata": {},
   "source": [
    "# upload repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a794ffe-6525-4f24-8de8-57a9db8321d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"XXXX/Fino1_Reasoning_FinQA\")\n",
    "\n",
    "# 2. 将需要的切分（例如 train）转换为 Pandas DataFrame \n",
    "#    注意：具体使用哪个 split，要根据实际情况来改，例如 'train'、'test' 或 'validation'\n",
    "df_fino1_reasoning = dataset['train'].to_pandas()\n",
    "\n",
    "df_filtered1['covered'] = df_filtered1['Open-ended Verifiable Question'].isin(\n",
    "    df_fino1_reasoning['Open-ended Verifiable Question']\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6af1d0-5d0f-4704-a134-41e6590e4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 先给所有行加上一列 flag，默认值为 0\n",
    "df_filtered1[\"flag\"] = 0\n",
    "\n",
    "# 在 cover=0 的行中，随机抽取 1000 条索引\n",
    "random_indices = df_filtered1[df_filtered1[\"covered\"] == 0] \\\n",
    "    .sample(n=1000, random_state=42).index\n",
    "\n",
    "# 将抽取出来的这 1000 行对应的 flag 设置为 1\n",
    "df_filtered1.loc[random_indices, \"flag\"] = 1\n",
    "\n",
    "# 创建一个新数据集，仅包含 flag=1 的行\n",
    "df_filtered1_random = df_filtered1[df_filtered1[\"flag\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699fad2-4a88-4d4d-9527-f5ad4454dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered1_1 = df_filtered1[df_filtered1.covered==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e4ffa-aaf4-487b-b60f-e6f9eb872e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered1_2 = df_filtered1_1[df_filtered1_1.flag==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde3552-2ef7-4372-8eb9-41bea9f4ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_name = 'XXXX/reasoning_data'\n",
    "\n",
    "\n",
    "# 创建 API 实例\n",
    "api = HfApi()\n",
    "\n",
    "# 创建数据集仓库\n",
    "dataset_repo_id = repo_name  # 替换为你的命名空间\n",
    "api.create_repo(repo_id=dataset_repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "print(f\"数据集仓库 {dataset_repo_id} 创建成功！\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "parquet_filename = \"train.parquet\"\n",
    "df_filtered1_2.to_parquet(parquet_filename, engine=\"pyarrow\")  # 确保使用 Parquet 格式\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "dataset_repo_id = repo_name\n",
    "\n",
    "# 上传 Parquet 文件\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_filename,\n",
    "    path_in_repo=\"train/train.parquet\",  # 保证它是 train split\n",
    "    repo_id=dataset_repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "print(\"Parquet 文件上传成功！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6046af-0658-4773-9992-0af2b8685529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 读取第一个文件\n",
    "with open(\"raw_reasoning_data_v2_3_CoT_search_472_1.json\", \"r\", encoding=\"utf-8\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "\n",
    "# 读取第二个文件\n",
    "with open(\"raw_reasoning_data_v2_3_CoT_search_481_2.json\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# 合并数据\n",
    "merged_data = data1 + data2\n",
    "\n",
    "# 保存为新文件\n",
    "with open(\"merged_reasoning_data.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(merged_data, fout, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ 合并完成，保存为 merged_reasoning_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a756d-9494-4442-8d28-5c0d4a86c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import HfApi, HfFolder, upload_folder\n",
    "import os\n",
    "\n",
    "# Step 1: 加载原始数据集\n",
    "dataset = load_dataset(\"xxxxxx/XXXX_reasoning\")\n",
    "\n",
    "# Step 2: 添加新列 \"answer\"（复制自 \"Ground-True Answer\"）\n",
    "def add_answer_column(example):\n",
    "    example['query'] = example[\"Open-ended Verifiable Question\"]\n",
    "    example[\"answer\"] = example[\"Ground-True Answer\"]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_answer_column)\n",
    "\n",
    "# Step 3: 保存为 Parquet 格式\n",
    "save_dir = \"reasoning_path_v2_all_parquet\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for split in dataset.keys():\n",
    "    dataset[split].to_parquet(os.path.join(save_dir, f\"{split}.parquet\"))\n",
    "\n",
    "# Step 4: 上传到 Hugging Face Hub（private dataset repo）\n",
    "repo_id = \"XXXXXX/reasoning_path_v2\"\n",
    "\n",
    "# 创建私有 repo（如果已经存在会跳过）\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=repo_id, private=True, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "# 上传文件夹\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=save_dir,\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(f\"✅ 数据集已成功上传为 Parquet 格式到：https://huggingface.co/datasets/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eea5b3-a3b9-4950-8c14-18cf2de46133",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417570dc-2a9a-4489-b3b1-fa45958c3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('formated_filtered_samples_data_filter_2025-03-13T18-15-17.353081.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da04d97-25f1-4535-85aa-88bedd1e7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "df_5000 = pd.DataFrame(load_dataset('XXXXXX/Fino1_Reasoning_FinQA')['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d0ab2-a780-4edf-88c7-ac2f813744d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_5000,df[['Open-ended Verifiable Question', 'Ground-True Answer', 'Complex_CoT', 'Response']]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc33a2-60b5-45b1-be77-3149a70ff800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaec3b6-bb72-4c48-9278-973ee4c22a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_json(\"combined_reasoning.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6de8c8-3e15-46ed-a772-fcb53c3ace7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLAMA",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
